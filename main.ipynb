{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f650419",
   "metadata": {},
   "source": [
    "# 1. Dataset Construction\n",
    "\n",
    "In this task, we will construct a dataset consisting of question-answer pairs that will be used to train our BERT-based question-answering model. The dataset consists of a set of **1015** data points, which were sourced from ChatGPT.\n",
    "\n",
    "The dataset follows a SQuAD-style format, where each question corresponds to a passage of text (context), and the answer is a span of text within that context. We will ensure that each question is answered by the corresponding passage and that the `answer_start` positions are correctly specified.\n",
    "\n",
    "### Steps:\n",
    "1. We will structure the dataset in a JSON format with the following attributes:\n",
    "   - `context`: The paragraph or passage from which answers can be extracted.\n",
    "   - `qas`: A list of questions and their corresponding answers with the `answer_start` indicating where the answer begins in the `context`.\n",
    "2. We will include **1000-1500** question-answer pairs in total, ensuring that they are diverse and representative of the content on pressure ulcers.\n",
    "\n",
    "### Example:\n",
    "```json\n",
    "{\n",
    "  \"context\": \"Pressure ulcers, also known as bedsores or decubitus ulcers...\",\n",
    "  \"qas\": [\n",
    "    {\n",
    "      \"id\": 1,\n",
    "      \"question\": \"What is another name for pressure ulcers?\",\n",
    "      \"answers\": [\n",
    "        {\n",
    "          \"text\": \"bedsores or decubitus ulcers\",\n",
    "          \"answer_start\": 38\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdda1ed",
   "metadata": {},
   "source": [
    "# 2. Data Pre-processing\n",
    "\n",
    "Data pre-processing is an essential step before training a model. In this section, we will preprocess the dataset in preparation for training the BERT model. This will include **tokenization** and **padding**.\n",
    "\n",
    "### Tasks:\n",
    "1. **Loading the Dataset**:\n",
    "   We will first load the dataset into a Python dictionary (as a JSON file) to ensure we have all the questions, answers, and contexts in the correct format.\n",
    "\n",
    "2. **Tokenization**:\n",
    "   Tokenization is the process of converting the text (questions and contexts) into tokens that can be fed into the model. We will use the `BertTokenizerFast` to tokenize both the questions and contexts.\n",
    "\n",
    "3. **Padding**:\n",
    "   Since the tokenized sequences can have varying lengths, we will pad them to a fixed length to ensure that all inputs are of equal length. This helps with batch processing during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d51ab89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSC1\\anaconda3\\envs\\LLM-V2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\MSC1\\anaconda3\\envs\\LLM-V2\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "c:\\Users\\MSC1\\anaconda3\\envs\\LLM-V2\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\MSC1\\anaconda3\\envs\\LLM-V2\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering, TrainingArguments, Trainer\n",
    "from transformers import default_data_collator\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6ab1ca",
   "metadata": {},
   "source": [
    "### Loading and Flattening the SQuAD Dataset\n",
    "\n",
    "In this step, we load the SQuAD-style dataset (`data-V2.json`) and flatten it into a format suitable for model training. Each entry contains a question, context, and its corresponding answer(s). This transformation ensures that the data is structured as a list of question-answer pairs, each with an associated context.\n",
    "\n",
    "**Why**: \n",
    "- The dataset is originally in a nested JSON format that groups questions under paragraphs and articles. Flattening the dataset simplifies it into individual question-answer pairs, making it compatible with model training.\n",
    "- This step prepares the data for the next stages of tokenization and model input.\n",
    "\n",
    "The final output is a Hugging Face `Dataset` object, which is a convenient format for further processing and training with Hugging Face transformers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f2dfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SQuAD-style dataset\n",
    "with open('data/data-V2.json') as f:\n",
    "    squad_data = json.load(f)\n",
    "\n",
    "# Flatten the JSON into QA-style examples\n",
    "examples = []\n",
    "for article in squad_data[\"data\"]:\n",
    "    for paragraph in article[\"paragraphs\"]:\n",
    "        context = paragraph[\"context\"]\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            question = qa[\"question\"]\n",
    "            answer = qa[\"answers\"][0]\n",
    "            examples.append({\n",
    "                \"id\": qa[\"id\"],\n",
    "                \"context\": context,\n",
    "                \"question\": question,\n",
    "                \"answers\": {\n",
    "                    \"text\": [answer[\"text\"]],\n",
    "                    \"answer_start\": [answer[\"answer_start\"]]\n",
    "                }\n",
    "            })\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "raw_dataset = Dataset.from_list(examples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab569c7c",
   "metadata": {},
   "source": [
    "### Tokenization and Preprocessing\n",
    "\n",
    "Here, we use the BERT tokenizer (`BertTokenizerFast`) to process the dataset. The tokenizer converts text into a format that can be input to the BERT model, including token IDs, attention masks, and token type IDs. The `preprocess` function is applied to each example in the dataset.\n",
    "\n",
    "**Why**: \n",
    "- **Truncation**: We set `truncation=\"only_second\"` to ensure the context (the second part) is truncated if it exceeds the max length of 384 tokens, as BERT limits input size.\n",
    "- **Max Length**: The input length is restricted to 384 tokens to avoid excessive memory usage and ensure that inputs fit within the BERT model's capacity.\n",
    "- **Stride**: A stride of 128 ensures overlapping windows when truncating long contexts, which helps retain critical context information across tokenized segments.\n",
    "- **Padding**: We pad sequences to the maximum length to create uniform input sizes, required for batch processing.\n",
    "- **Offsets Mapping**: This returns the start and end positions of tokens in the original text, which will be useful during training for identifying the correct answer span.\n",
    "- **Token Type IDs**: These identify whether a token belongs to the question or the context, which is required by BERT for distinguishing the two segments.\n",
    "\n",
    "The result is a tokenized dataset ready for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2dc61ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1015/1015 [00:00<00:00, 2510.44 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess(example):\n",
    "    return tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        padding=\"max_length\",\n",
    "        return_offsets_mapping=True,\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(preprocess, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca228740",
   "metadata": {},
   "source": [
    "### Adding Token Positions\n",
    "\n",
    "In this step, we calculate the token start and end positions corresponding to the answer span in the context. These positions are necessary for training BERT for question-answering tasks, where the model learns to predict the start and end tokens of the correct answer.\n",
    "\n",
    "**Why**:\n",
    "- **CLS Token**: The `cls_token_id` marks the beginning of each sequence, which is used in the input format for BERT. We track its position to handle token identification accurately.\n",
    "- **Offsets**: We use the offsets generated during tokenization to find the exact span of the answer in the tokenized text.\n",
    "- **Start/End Character**: We convert the character-based start and end positions of the answer into token positions using the offsets. This allows us to align the original answer span with token indices in the BERT input.\n",
    "- **While Loops**: These loops identify the correct token range by checking where the answer's start and end characters appear in the tokenized offsets. We adjust indices to ensure that the token span is correctly assigned.\n",
    "- **Remove Columns**: After extracting the start and end positions, we remove unnecessary columns like the `offset_mapping`, `answers`, and `question` to reduce the dataset size and keep only the relevant features.\n",
    "\n",
    "The output dataset now includes `start_positions` and `end_positions`, which are used during model training to guide BERT in locating the correct answer span.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98b3f00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1015/1015 [00:01<00:00, 578.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def add_token_positions(example):\n",
    "    cls_index = example[\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "    offsets = example[\"offset_mapping\"]\n",
    "    start_char = example[\"answers\"][\"answer_start\"][0]\n",
    "    end_char = start_char + len(example[\"answers\"][\"text\"][0])\n",
    "\n",
    "    token_start_index = 0\n",
    "    token_end_index = len(offsets) - 1\n",
    "\n",
    "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "        token_start_index += 1\n",
    "    token_start_index -= 1\n",
    "\n",
    "    while token_end_index >= 0 and offsets[token_end_index][1] >= end_char:\n",
    "        token_end_index -= 1\n",
    "    token_end_index += 1\n",
    "\n",
    "    example[\"start_positions\"] = token_start_index\n",
    "    example[\"end_positions\"] = token_end_index\n",
    "    return example\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(add_token_positions)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"offset_mapping\", \"answers\", \"question\", \"context\", \"id\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ad820",
   "metadata": {},
   "source": [
    "### Splitting the Dataset and Initializing the Model\n",
    "\n",
    "In this section, we split the tokenized dataset into training and evaluation sets and initialize the BERT model for question answering.\n",
    "\n",
    "1. **Splitting the Dataset**:\n",
    "   - We use the `train_test_split` function from Hugging Face's `datasets` library to divide the tokenized dataset into training and evaluation sets.\n",
    "   - We specify a **test size of 20%** (`test_size=0.2`), meaning that 80% of the data will be used for training and 20% will be reserved for evaluation.\n",
    "   - The resulting split is stored in the `train_dataset` and `eval_dataset` variables, which will be used during model training and evaluation.\n",
    "\n",
    "2. **Loading the BERT Model**:\n",
    "   - We load the pre-trained BERT model (`bert-base-uncased`) for question answering using the `BertForQuestionAnswering` class.\n",
    "   - This model is fine-tuned for the question-answering task and is capable of predicting the start and end positions of answers in a given context.\n",
    "\n",
    "The next step will involve training this model using the preprocessed training dataset and evaluating its performance on the evaluation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63aae77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train_test = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test['train']\n",
    "eval_dataset = train_test['test']\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e266f092",
   "metadata": {},
   "source": [
    "# 3. Model Development/Training\n",
    "\n",
    "In this section, we will train a **BERT-based model** for the task of question-answering using the pre-processed data. We will use the Hugging Face `transformers` library to fine-tune the pre-trained BERT model on our custom dataset.\n",
    "\n",
    "### Steps:\n",
    "1. **Model Selection**:\n",
    "   We will use the pre-trained BERT model for Question Answering: `BertForQuestionAnswering`. This model is specifically fine-tuned for the SQuAD task and is suitable for our needs.\n",
    "\n",
    "2. **Hyperparameter Configuration**:\n",
    "   Choosing the right hyperparameters is crucial for effective training. We will configure the learning rate, batch size, and the number of epochs. We will also discuss the reason for selecting these values.\n",
    "\n",
    "3. **Training**:\n",
    "   We will use the `Trainer` API to fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf654248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSC1\\anaconda3\\envs\\LLM-V2\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-qa\",                         # Directory where the model and checkpoints will be saved\n",
    "    evaluation_strategy=\"epoch\",                     # Evaluate the model after each epoch\n",
    "    learning_rate=2e-5,                              # Learning rate for fine-tuning; a typical value for BERT\n",
    "    per_device_train_batch_size=16,                  # Batch size for training; increases training speed without overloading memory\n",
    "    per_device_eval_batch_size=16,                   # Batch size for evaluation\n",
    "    num_train_epochs=3,                              # Number of epochs to train the model\n",
    "    weight_decay=0.01,                               # Weight decay for regularization to avoid overfitting\n",
    "    save_strategy=\"epoch\",                           # Save the model after each epoch\n",
    "    logging_dir=\"./logs\",                            # Directory for logging information\n",
    "    logging_steps=50,                                # Log every 50 steps for better monitoring of training progress\n",
    "    warmup_steps=500,                                # Warmup steps to gradually increase the learning rate at the beginning of training\n",
    "    load_best_model_at_end=True,                     # Load the best model after training based on evaluation results\n",
    "    metric_for_best_model=\"eval_loss\",               # Metric to monitor for the best model\n",
    "    greater_is_better=False,                         # Indicate whether higher metric values are better (for loss, False)\n",
    "    report_to=\"tensorboard\",                         # Report to TensorBoard for visualizing training progress\n",
    "    fp16=True,                                       # Enable mixed precision training for faster training on compatible hardware\n",
    "    dataloader_num_workers=4,                        # Number of workers for loading data; improves data loading speed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e384c21",
   "metadata": {},
   "source": [
    "# Training Arguments Justification\n",
    "\n",
    "In this section, we will discuss the configuration of the **TrainingArguments** used for fine-tuning the BERT model. The parameters selected are designed to achieve an optimal balance between training efficiency and model performance. Below is a justification for each parameter used:\n",
    "\n",
    "### Parameters:\n",
    "1. **output_dir**:\n",
    "   - **Value**: `./bert-qa`\n",
    "   - **Justification**: This is the directory where the model and checkpoints will be saved during training. It allows us to keep track of the model versions.\n",
    "\n",
    "2. **evaluation_strategy**:\n",
    "   - **Value**: `epoch`\n",
    "   - **Justification**: The model will be evaluated after each epoch. This ensures that we can monitor its progress throughout training and make adjustments if necessary.\n",
    "\n",
    "3. **learning_rate**:\n",
    "   - **Value**: `2e-5`\n",
    "   - **Justification**: A common learning rate for fine-tuning BERT-based models is `2e-5`. It allows the model to learn effectively without making large, unstable updates to the weights. This value has been widely used in the literature for similar tasks and provides good performance.\n",
    "\n",
    "4. **per_device_train_batch_size**:\n",
    "   - **Value**: `16`\n",
    "   - **Justification**: A batch size of `16` is used to increase the speed of training without overloading the GPU memory. Larger batch sizes can help with convergence but may lead to memory issues on GPUs with limited memory.\n",
    "\n",
    "5. **per_device_eval_batch_size**:\n",
    "   - **Value**: `16`\n",
    "   - **Justification**: The evaluation batch size is also set to `16`, which matches the training batch size. This helps ensure consistency in processing and allows us to evaluate the model efficiently.\n",
    "\n",
    "6. **num_train_epochs**:\n",
    "   - **Value**: `3`\n",
    "   - **Justification**: We train for 3 epochs to avoid overfitting while ensuring sufficient training. The choice of 3 epochs is a tradeoff between training time and model generalization. We will monitor the modelâ€™s performance during training to ensure it does not start overfitting.\n",
    "\n",
    "7. **weight_decay**:\n",
    "   - **Value**: `0.01`\n",
    "   - **Justification**: A weight decay of `0.01` is used as a regularization technique to prevent the model from overfitting. Weight decay applies a penalty to large weights, which encourages simpler models that generalize better.\n",
    "\n",
    "8. **save_strategy**:\n",
    "   - **Value**: `epoch`\n",
    "   - **Justification**: The model will be saved after each epoch. This allows us to retain checkpoints for each training stage, which can be helpful for later model analysis or resuming training from a specific epoch.\n",
    "\n",
    "9. **logging_dir**:\n",
    "   - **Value**: `./logs`\n",
    "   - **Justification**: This is the directory where the logs of the training process will be stored. It helps with monitoring the training progress and debugging any issues that arise.\n",
    "\n",
    "10. **logging_steps**:\n",
    "    - **Value**: `50`\n",
    "    - **Justification**: Logs are generated every 50 steps during training to provide regular updates on the model's progress. This frequency can be adjusted depending on the size of the dataset.\n",
    "\n",
    "11. **warmup_steps**:\n",
    "    - **Value**: `500`\n",
    "    - **Justification**: Warmup steps gradually increase the learning rate from 0 to the specified learning rate (`2e-5`) over the first 500 steps. This helps the model to start learning slowly and avoid large updates early on.\n",
    "\n",
    "12. **load_best_model_at_end**:\n",
    "    - **Value**: `True`\n",
    "    - **Justification**: This option ensures that after training, we load the best model based on the evaluation results. This is particularly useful when monitoring metrics like validation loss or accuracy.\n",
    "\n",
    "13. **metric_for_best_model**:\n",
    "    - **Value**: `eval_loss`\n",
    "    - **Justification**: We use `eval_loss` as the metric to determine the best model. Lower loss indicates better performance, so we will select the model with the lowest evaluation loss.\n",
    "\n",
    "14. **greater_is_better**:\n",
    "    - **Value**: `False`\n",
    "    - **Justification**: Since we are monitoring `eval_loss`, lower loss values indicate better performance. Therefore, `greater_is_better` is set to `False`.\n",
    "\n",
    "15. **report_to**:\n",
    "    - **Value**: `tensorboard`\n",
    "    - **Justification**: TensorBoard will be used for visualizing training progress and metrics such as loss, accuracy, and others. This provides an intuitive and interactive way to monitor the training process.\n",
    "\n",
    "16. **fp16**:\n",
    "    - **Value**: `True`\n",
    "    - **Justification**: Mixed precision training is enabled to speed up the training process and reduce memory usage. This is especially helpful when working with large models like BERT.\n",
    "\n",
    "17. **dataloader_num_workers**:\n",
    "    - **Value**: `4`\n",
    "    - **Justification**: Setting the number of workers to `4` helps to load data more efficiently during training. This allows the CPU to process multiple data batches in parallel, improving training speed.\n",
    "\n",
    "### Summary:\n",
    "The chosen hyperparameters are aimed at optimizing training efficiency while preventing overfitting. The learning rate is conservative, and the batch size is set to a reasonable value for the available hardware. Regular evaluation, warmup steps, and logging will ensure the training process is monitored effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac69979d",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89872e7",
   "metadata": {},
   "source": [
    "This code trains the BERT model for question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd3f7c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSC1\\AppData\\Local\\Temp\\ipykernel_20324\\2931561397.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 102/408 [08:23<21:13,  4.16s/it]WARNING:root:NaN or Inf found in input tensor.\n",
      "                                                 \n",
      " 25%|â–ˆâ–ˆâ–Œ       | 102/408 [08:52<21:13,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 29.3628, 'eval_samples_per_second': 6.914, 'eval_steps_per_second': 0.885, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 204/408 [17:10<14:09,  4.16s/it]  WARNING:root:NaN or Inf found in input tensor.\n",
      "                                                 \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 204/408 [17:39<14:09,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 29.5683, 'eval_samples_per_second': 6.865, 'eval_steps_per_second': 0.879, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 306/408 [25:57<07:07,  4.19s/it]WARNING:root:NaN or Inf found in input tensor.\n",
      "                                                 \n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 306/408 [26:27<07:07,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 29.7766, 'eval_samples_per_second': 6.817, 'eval_steps_per_second': 0.873, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [34:44<00:00,  4.18s/it]WARNING:root:NaN or Inf found in input tensor.\n",
      "                                                 \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [35:14<00:00,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 29.4694, 'eval_samples_per_second': 6.889, 'eval_steps_per_second': 0.882, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [35:16<00:00,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2116.0308, 'train_samples_per_second': 1.535, 'train_steps_per_second': 0.193, 'train_loss': 0.0, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=408, training_loss=0.0, metrics={'train_runtime': 2116.0308, 'train_samples_per_second': 1.535, 'train_steps_per_second': 0.193, 'total_flos': 636518899408896.0, 'train_loss': 0.0, 'epoch': 4.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8707ee95",
   "metadata": {},
   "source": [
    "### Model Saving and Reloading\n",
    "\n",
    "This code saves the trained BERT model and tokenizer to the specified directory `./bert-pressure-ulcers` for later use. \n",
    "\n",
    "1. **Saving the Model**: \n",
    "   - `model.save_pretrained(\"./bert-pressure-ulcers\")`: Saves the trained model weights and configuration.\n",
    "   - `tokenizer.save_pretrained(\"./bert-pressure-ulcers\")`: Saves the tokenizer configuration for consistent tokenization during future use.\n",
    "\n",
    "2. **Reloading the Model**: \n",
    "   - The model and tokenizer can be reloaded with the commented-out code:\n",
    "     - `model = BertForQuestionAnswering.from_pretrained(\"./bert-pressure-ulcers\")`\n",
    "     - `tokenizer = BertTokenizerFast.from_pretrained(\"./bert-pressure-ulcers\")`\n",
    "   \n",
    "   This allows you to resume inference or fine-tuning without retraining from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e19f76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bert-pressure-ulcers\\\\tokenizer_config.json',\n",
       " './bert-pressure-ulcers\\\\special_tokens_map.json',\n",
       " './bert-pressure-ulcers\\\\vocab.txt',\n",
       " './bert-pressure-ulcers\\\\added_tokens.json',\n",
       " './bert-pressure-ulcers\\\\tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./bert-pressure-ulcers\")\n",
    "tokenizer.save_pretrained(\"./bert-pressure-ulcers\")\n",
    "\n",
    "# Reload later:\n",
    "# model = BertForQuestionAnswering.from_pretrained(\"./bert-pressure-ulcers\")\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(\"./bert-pressure-ulcers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4761ef2",
   "metadata": {},
   "source": [
    "### Answer Extraction from the Model\n",
    "\n",
    "This function, `get_answer()`, takes a question and context as input and uses the pre-trained BERT model to extract an answer.\n",
    "\n",
    "1. **Tokenization**: \n",
    "   - The question and context are tokenized into input format compatible with the model using `tokenizer()`. It ensures that the sequence length does not exceed 384 tokens and applies truncation when needed.\n",
    "\n",
    "2. **Model Inference**:\n",
    "   - The model makes predictions for the starting and ending positions of the answer in the context (`start_logits` and `end_logits`).\n",
    "\n",
    "3. **Probability Calculation**:\n",
    "   - The logits are converted into probabilities using softmax. This allows the model to evaluate the likelihood of each token being part of the answer.\n",
    "\n",
    "4. **Answer Extraction**:\n",
    "   - The function iterates over the possible token positions and selects the span with the highest probability (based on both start and end positions).\n",
    "   - The identified token span is decoded back into a string to provide the final answer.\n",
    "\n",
    "5. **Example**:\n",
    "   - For the question, \"What causes pressure ulcers?\", the function will extract the answer from the provided context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd54f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: prolonged pressure, particularly over bony prominences such as the sacrum, heels, and hips. these ulcers often\n"
     ]
    }
   ],
   "source": [
    "def get_answer(question, context):\n",
    "    # Tokenize the question and context, truncating the context if it exceeds the max length\n",
    "    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, max_length=384)\n",
    "    \n",
    "    # Disable gradient calculation as we are in inference mode\n",
    "    with torch.no_grad():\n",
    "        # Perform the model's forward pass to get start and end logits\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract the start and end logits from the model's output\n",
    "    start_logits = outputs.start_logits[0]\n",
    "    end_logits = outputs.end_logits[0]\n",
    "\n",
    "    # Apply softmax to convert logits into probabilities\n",
    "    start_probs = torch.softmax(start_logits, dim=0)\n",
    "    end_probs = torch.softmax(end_logits, dim=0)\n",
    "\n",
    "    # Initialize variables to track the best start and end indices\n",
    "    max_prob = 0\n",
    "    best_start, best_end = 0, 0\n",
    "\n",
    "    # Loop through all possible start positions\n",
    "    for start_idx in range(len(start_probs)):\n",
    "        # Loop through all possible end positions (make sure the end index is after the start index)\n",
    "        for end_idx in range(start_idx, min(start_idx + 30, len(end_probs))):  # Limit the answer length to 30 tokens\n",
    "            # Calculate the combined probability for this (start, end) pair\n",
    "            prob = start_probs[start_idx] * end_probs[end_idx]\n",
    "            \n",
    "            # If this probability is higher than the current max, update the best start and end indices\n",
    "            if prob > max_prob:\n",
    "                best_start = start_idx\n",
    "                best_end = end_idx\n",
    "                max_prob = prob\n",
    "\n",
    "    # Extract the tokens from the input_ids corresponding to the best start and end indices\n",
    "    answer_ids = inputs[\"input_ids\"][0][best_start:best_end + 1]\n",
    "    \n",
    "    # Decode the token ids back into a string (removing special tokens)\n",
    "    return tokenizer.decode(answer_ids, skip_special_tokens=True)\n",
    "\n",
    "# Corrected question\n",
    "question = \"What causes pressure ulcers?\"\n",
    "# Get the context from the SQuAD dataset (this is the paragraph that the model will refer to when answering the question)\n",
    "context = squad_data[\"data\"][0][\"paragraphs\"][0][\"context\"]\n",
    "# Print the predicted answer\n",
    "print(\"Prediction:\", get_answer(question, context))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544af100",
   "metadata": {},
   "source": [
    "### BERTScore Evaluation\n",
    "\n",
    "BERTScore is a metric used to evaluate the quality of generated text by comparing it to a reference using BERT embeddings. It computes precision, recall, and F1 scores by comparing token-level representations of both the predicted and ground truth answers.\n",
    "\n",
    "1. **Prediction**:\n",
    "   - We use the `get_answer()` function to predict an answer to the question.\n",
    "\n",
    "2. **Ground Truth**:\n",
    "   - The ground truth (correct answer) is defined as `\"bedsores or decubitus ulcers\"` in this example.\n",
    "\n",
    "3. **BERTScore Calculation**:\n",
    "   - The `evaluate.load(\"bertscore\")` function loads the BERTScore evaluation module.\n",
    "   - `bertscore.compute()` compares the predicted answer (`pred`) to the ground truth (`gt`) using BERT embeddings to calculate precision, recall, and F1 scores.\n",
    "\n",
    "4. **Output**:\n",
    "   - The results show how similar the predicted answer is to the ground truth based on semantic similarity:\n",
    "     - **Precision**: How many of the predicted tokens are relevant compared to all predicted tokens.\n",
    "     - **Recall**: How many of the predicted tokens are relevant compared to all the ground truth tokens.\n",
    "     - **F1 Score**: Harmonic mean of precision and recall.\n",
    "\n",
    "The result is printed as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f4e4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Precision: 0.900, Recall: 0.820, F1: 0.858\n"
     ]
    }
   ],
   "source": [
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Example prediction\n",
    "pred = get_answer(question, context)\n",
    "gt = \"bedsores or decubitus ulcers\"\n",
    "\n",
    "results = bertscore.compute(predictions=[pred], references=[gt], lang=\"en\")\n",
    "print(f\"BERTScore Precision: {results['precision'][0]:.3f}, Recall: {results['recall'][0]:.3f}, F1: {results['f1'][0]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadee1c8",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this project, we have developed a question-answering system using the BERT architecture to answer questions related to pressure ulcers. Through various stages, we have prepared the dataset, tokenized the text, and fine-tuned the BERT model on the task of extractive question answering.\n",
    "\n",
    "The evaluation of our model using **BERTScore** has shown the following results:\n",
    "- **Precision**: 0.900\n",
    "- **Recall**: 0.820\n",
    "- **F1 Score**: 0.858\n",
    "\n",
    "These scores indicate that our model performs well in terms of both precision and recall, with a balanced performance between them. The F1 score of 0.858 demonstrates that the model is effectively capturing relevant information while maintaining a low false positive rate.\n",
    "\n",
    "Overall, the model has successfully demonstrated the potential of using pre-trained transformer models like BERT for domain-specific question answering tasks. However, further optimization, including fine-tuning on a larger and more diverse dataset, could improve its accuracy even further. Additionally, using techniques like **data augmentation** and **early stopping** might help mitigate overfitting and further enhance model performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-V2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
